
train_path: data/dtrainECNC.json
# eval_path: numeval_task3/data/ddevR.json
task: rationale_hg_stg1_ecnc


model_id: mistralai/Mistral-7B-v0.3  #meta-llama/Meta-Llama-3.1-8B #mistralai/Mistral-7B-v0.3
tokenizer_model_max_length: 2048
tokenizer_padding_side: right
tokenizer_pad_token: '[PAD]'


max_seq_length: 2048
packing: true
max_grad_norm: 0.3
bf16: true
tf32: true
# do_eval: true
# eval_strategy: epoch
gradient_accumulation_steps: 8
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: false
learning_rate: 0.0002
log_level: info
logging_steps: 10
logging_strategy: steps
lr_scheduler_type: cosine
max_steps: -1
num_train_epochs: 3
output_dir: results/models/numhg/ecnc/mistral/stg1-sft-r128a64lr2e4ep3x
overwrite_output_dir: true
# per_device_eval_batch_size: 8
per_device_train_batch_size: 4
save_strategy: epoch
save_total_limit: 1
seed: 42
warmup_ratio: 0.1
# load_best_model_at_end: true
dataset_kwargs:
  add_special_tokens: false
  append_concat_token: false 
report_to: wandb

r: 128
lora_alpha: 64
lora_dropout: 0.05
bias: none
task_type: CAUSAL_LM
target_modules:
- q_proj
- k_proj
- v_proj
- o_proj
- gate_proj
- up_proj
- down_proj
